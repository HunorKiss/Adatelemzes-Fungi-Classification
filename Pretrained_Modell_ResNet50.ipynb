{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkOqNL6okDHm"
   },
   "source": [
    " Import Necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 8302,
     "status": "ok",
     "timestamp": 1733743690206,
     "user": {
      "displayName": "Házifeladat Adatelemzés",
      "userId": "03777937392997268846"
     },
     "user_tz": -60
    },
    "id": "e1BE4LUQT23H"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import zipfile\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.layers import AveragePooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.applications import ResNet50\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from sklearn.metrics import classification_report\n",
    "from imutils import paths\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import argparse\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "executionInfo": {
     "elapsed": 19743,
     "status": "ok",
     "timestamp": 1733743709945,
     "user": {
      "displayName": "Házifeladat Adatelemzés",
      "userId": "03777937392997268846"
     },
     "user_tz": -60
    },
    "id": "N7AnhifpNjAp"
   },
   "outputs": [],
   "source": [
    "from PIL import Image, ImageFile\n",
    "from torchvision import datasets, transforms, models\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch import nn, optim\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b3jUzaEqkIUr"
   },
   "source": [
    "Mount Google Drive and Extract ZIP file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 98826,
     "status": "ok",
     "timestamp": 1733743808755,
     "user": {
      "displayName": "Házifeladat Adatelemzés",
      "userId": "03777937392997268846"
     },
     "user_tz": -60
    },
    "id": "QqOWGlRXXkXR",
    "outputId": "23b46b39-3812-4172-ad5c-ab9e6982fe7d"
   },
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "\n",
    "# Mount Google Drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "# Define the path to the ZIP file in your Google Drive\n",
    "root_dir = 'top_100_species_images/top_100_species_images'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gceJK-Bfk6HU"
   },
   "source": [
    "Load Images and Extract Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5213,
     "status": "ok",
     "timestamp": 1733743813964,
     "user": {
      "displayName": "Házifeladat Adatelemzés",
      "userId": "03777937392997268846"
     },
     "user_tz": -60
    },
    "id": "HvT91vRCaJxA",
    "outputId": "7baba173-14ce-450e-c564-18568fc495b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final dataframe shape: (84969, 2)\n",
      "Number of unique species: 100\n",
      "['Trametes versicolor', 'Fomitopsis pinicola', 'Hypholoma fasciculare', 'Pluteus cervinus', 'Fomes fomentarius', 'Bjerkandera adusta', 'Auricularia auricula-judae', 'Coprinellus micaceus', 'Stereum hirsutum', 'Mycena galericulata', 'Plicaturopsis crispa', 'Cuphophyllus virgineus', 'Hygrocybe miniata', 'Clitocybe nebularis', 'Ganoderma applanatum', 'Daedaleopsis confragosa', 'Pleurotus ostreatus', 'Tremella mesenterica', 'Gymnopilus penetrans', 'Meripilus giganteus', 'Armillaria lutea', 'Amanita muscaria', 'Tubaria furfuracea', 'Leccinum scabrum', 'Neoboletus luridiformis', 'Imleria badia', 'Lycoperdon perlatum', 'Phaeolepiota aurea', 'Trametes hirsuta', 'Trametes gibbosa', 'Boletus edulis', 'Pseudocraterellus undulatus', 'Byssomerulius corium', 'Schizophyllum commune', 'Kuehneromyces mutabilis', 'Xylaria hypoxylon', 'Daedalea quercina', 'Laccaria amethystina', 'Xerocomellus chrysenteron', 'Coprinus comatus', 'Phlebia tremellosa', 'Parmelia sulcata', 'Lepista nuda', 'Fomitopsis betulina', 'Laccaria laccata', 'Cerioporus varius', 'Hygrocybe ceracea', 'Phlebia radiata', 'Psathyrella candolleana', 'Amanita rubescens', 'Chondrostereum purpureum', 'Cerioporus squamosus', 'Hymenopellis radicata', 'Xanthoria parietina', 'Lentinus brumalis', 'Xerocomellus pruinatus', 'Hygrocybe conica', 'Exidia nigricans', 'Pholiota squarrosa', 'Gliophorus psittacinus', 'Phaeolus schweinitzii', 'Mucidula mucida', 'Paxillus involutus', 'Fuscoporia ferrea', 'Russula ochroleuca', 'Tricholoma scalpturatum', 'Cylindrobasidium evolvens', 'Phellinus pomaceus', 'Paralepista flaccida', 'Gloeophyllum sepiarium', 'Ramaria stricta', 'Suillellus luridus', 'Laetiporus sulphureus', 'Tricholoma terreum', 'Cantharellus cibarius', 'Russula adusta', 'Tricholomopsis rutilans', 'Dacrymyces stillatus', 'Mycetinis alliaceus', 'Lacrymaria lacrymabunda', 'Ganoderma pfeifferi', 'Scleroderma citrinum', 'Clitopilus prunulus', 'Panellus stipticus', 'Hygrocybe coccinea', 'Amanita fulva', 'Coprinopsis picacea', 'Cuphophyllus pratensis', 'Cystoderma amianthinum', 'Chlorophyllum olivieri', 'Hygrophoropsis aurantiaca', 'Coprinellus disseminatus', 'Phallus impudicus', 'Inocybe geophylla', 'Hypoxylon fragiforme', 'Mycena rosea', 'Bolbitius titubans', 'Exidia glandulosa', 'Xylodon radula', 'Jackrogersella multiformis']\n",
      "{'Trametes versicolor': 0, 'Fomitopsis pinicola': 1, 'Hypholoma fasciculare': 2, 'Pluteus cervinus': 3, 'Fomes fomentarius': 4, 'Bjerkandera adusta': 5, 'Auricularia auricula-judae': 6, 'Coprinellus micaceus': 7, 'Stereum hirsutum': 8, 'Mycena galericulata': 9, 'Plicaturopsis crispa': 10, 'Cuphophyllus virgineus': 11, 'Hygrocybe miniata': 12, 'Clitocybe nebularis': 13, 'Ganoderma applanatum': 14, 'Daedaleopsis confragosa': 15, 'Pleurotus ostreatus': 16, 'Tremella mesenterica': 17, 'Gymnopilus penetrans': 18, 'Meripilus giganteus': 19, 'Armillaria lutea': 20, 'Amanita muscaria': 21, 'Tubaria furfuracea': 22, 'Leccinum scabrum': 23, 'Neoboletus luridiformis': 24, 'Imleria badia': 25, 'Lycoperdon perlatum': 26, 'Phaeolepiota aurea': 27, 'Trametes hirsuta': 28, 'Trametes gibbosa': 29, 'Boletus edulis': 30, 'Pseudocraterellus undulatus': 31, 'Byssomerulius corium': 32, 'Schizophyllum commune': 33, 'Kuehneromyces mutabilis': 34, 'Xylaria hypoxylon': 35, 'Daedalea quercina': 36, 'Laccaria amethystina': 37, 'Xerocomellus chrysenteron': 38, 'Coprinus comatus': 39, 'Phlebia tremellosa': 40, 'Parmelia sulcata': 41, 'Lepista nuda': 42, 'Fomitopsis betulina': 43, 'Laccaria laccata': 44, 'Cerioporus varius': 45, 'Hygrocybe ceracea': 46, 'Phlebia radiata': 47, 'Psathyrella candolleana': 48, 'Amanita rubescens': 49, 'Chondrostereum purpureum': 50, 'Cerioporus squamosus': 51, 'Hymenopellis radicata': 52, 'Xanthoria parietina': 53, 'Lentinus brumalis': 54, 'Xerocomellus pruinatus': 55, 'Hygrocybe conica': 56, 'Exidia nigricans': 57, 'Pholiota squarrosa': 58, 'Gliophorus psittacinus': 59, 'Phaeolus schweinitzii': 60, 'Mucidula mucida': 61, 'Paxillus involutus': 62, 'Fuscoporia ferrea': 63, 'Russula ochroleuca': 64, 'Tricholoma scalpturatum': 65, 'Cylindrobasidium evolvens': 66, 'Phellinus pomaceus': 67, 'Paralepista flaccida': 68, 'Gloeophyllum sepiarium': 69, 'Ramaria stricta': 70, 'Suillellus luridus': 71, 'Laetiporus sulphureus': 72, 'Tricholoma terreum': 73, 'Cantharellus cibarius': 74, 'Russula adusta': 75, 'Tricholomopsis rutilans': 76, 'Dacrymyces stillatus': 77, 'Mycetinis alliaceus': 78, 'Lacrymaria lacrymabunda': 79, 'Ganoderma pfeifferi': 80, 'Scleroderma citrinum': 81, 'Clitopilus prunulus': 82, 'Panellus stipticus': 83, 'Hygrocybe coccinea': 84, 'Amanita fulva': 85, 'Coprinopsis picacea': 86, 'Cuphophyllus pratensis': 87, 'Cystoderma amianthinum': 88, 'Chlorophyllum olivieri': 89, 'Hygrophoropsis aurantiaca': 90, 'Coprinellus disseminatus': 91, 'Phallus impudicus': 92, 'Inocybe geophylla': 93, 'Hypoxylon fragiforme': 94, 'Mycena rosea': 95, 'Bolbitius titubans': 96, 'Exidia glandulosa': 97, 'Xylodon radula': 98, 'Jackrogersella multiformis': 99}\n"
     ]
    }
   ],
   "source": [
    "#### Load the dataset from CSV file\n",
    "df = pd.read_csv('FungiCLEF2023_train_metadata_PRODUCTION.csv')\n",
    "\n",
    "#### Find the top 100 species by number of examples\n",
    "top_100_species = df['species'].value_counts().head(100).index.tolist()\n",
    "\n",
    "#### Filter the dataframe to only include rows from the top 100 species\n",
    "df_top_100 = df[df['species'].isin(top_100_species)]\n",
    "\n",
    "#### Create a new dataframe with only the 'image_path' and 'class_id' columns\n",
    "top_100_pairs = df_top_100[['image_path', 'species']]\n",
    "\n",
    "#### Print the shape of the final dataframe\n",
    "print(f\"Final dataframe shape: {top_100_pairs.shape}\")\n",
    "\n",
    "unique_species_ids = top_100_pairs['species'].nunique()\n",
    "print(f\"Number of unique species: {unique_species_ids}\")\n",
    "print(top_100_species)\n",
    "\n",
    "# Create a dictionary that maps each species to a unique integer label\n",
    "species_labels = {species: index for index, species in enumerate(top_100_species)}\n",
    "\n",
    "print(species_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1733743813964,
     "user": {
      "displayName": "Házifeladat Adatelemzés",
      "userId": "03777937392997268846"
     },
     "user_tz": -60
    },
    "id": "Z5mfW2PqTQyz"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Define transformations to be applied to each image (resizing, converting to tensor, normalizing)\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize images to 288x288 pixels\n",
    "    transforms.ToTensor(),  # Convert image to a tensor (used by PyTorch)\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1733743813965,
     "user": {
      "displayName": "Házifeladat Adatelemzés",
      "userId": "03777937392997268846"
     },
     "user_tz": -60
    },
    "id": "Xd1pwuwfN5lJ"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "# Define a custom dataset class for loading our images and labels\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, root_dir, top_100_df, transform=None):\n",
    "        self.root_dir = root_dir\n",
    "        self.top_100_df = top_100_df\n",
    "        self.transform = transform\n",
    "        self.image_paths = []  # List to store image paths\n",
    "        self.labels = []       # List to store image labels\n",
    "        image_count = 0\n",
    "        label_count = 0\n",
    "        iterc = 0\n",
    "\n",
    "        print(\"Root directory:\", root_dir)  # Print the root directory\n",
    "\n",
    "        # Loop through each file in the root directory\n",
    "        for file_name in os.listdir(root_dir):\n",
    "            if file_name.endswith('.JPG') and not file_name.startswith('__MACOSX/'):\n",
    "\n",
    "                if(iterc % 5 == 0):\n",
    "                    # extract image file route\n",
    "                    image_path = os.path.join(root_dir, file_name)\n",
    "                    self.image_paths.append(image_path)  # Store the image path\n",
    "                    image_count += 1\n",
    "\n",
    "                    # Extract the label from the file name\n",
    "                    image_path_after_slash = os.path.basename(file_name)\n",
    "                    label = species_labels.get(self.top_100_df.loc[self.top_100_df['image_path'] == image_path_after_slash, 'species'].values[0])\n",
    "                    self.labels.append(label)  # Store the corresponding label\n",
    "                    label_count += 1\n",
    "\n",
    "                iterc += 1\n",
    "\n",
    "                if (iterc % 100) == 0:\n",
    "                    print(iterc)\n",
    "\n",
    "        print(len(set(self.labels)))\n",
    "        print(\"Root directory:\", root_dir)  # Print the root directory\n",
    "        print(\"Image count:\", image_count)\n",
    "        print(\"Label count:\", label_count)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 69435,
     "status": "ok",
     "timestamp": 1733743883396,
     "user": {
      "displayName": "Házifeladat Adatelemzés",
      "userId": "03777937392997268846"
     },
     "user_tz": -60
    },
    "id": "b-eMPx85jdGJ",
    "outputId": "1ecb2408-8e9f-434e-9ef5-a81d96c447fd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root directory: top_100_species_images/top_100_species_images\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n",
      "11800\n",
      "11900\n",
      "12000\n",
      "12100\n",
      "12200\n",
      "12300\n",
      "12400\n",
      "12500\n",
      "12600\n",
      "12700\n",
      "12800\n",
      "12900\n",
      "13000\n",
      "13100\n",
      "13200\n",
      "13300\n",
      "13400\n",
      "13500\n",
      "13600\n",
      "13700\n",
      "13800\n",
      "13900\n",
      "14000\n",
      "14100\n",
      "14200\n",
      "14300\n",
      "14400\n",
      "14500\n",
      "14600\n",
      "14700\n",
      "14800\n",
      "14900\n",
      "15000\n",
      "15100\n",
      "15200\n",
      "15300\n",
      "15400\n",
      "15500\n",
      "15600\n",
      "15700\n",
      "15800\n",
      "15900\n",
      "16000\n",
      "16100\n",
      "16200\n",
      "16300\n",
      "16400\n",
      "16500\n",
      "16600\n",
      "16700\n",
      "16800\n",
      "16900\n",
      "17000\n",
      "17100\n",
      "17200\n",
      "17300\n",
      "17400\n",
      "17500\n",
      "17600\n",
      "17700\n",
      "17800\n",
      "17900\n",
      "18000\n",
      "18100\n",
      "18200\n",
      "18300\n",
      "18400\n",
      "18500\n",
      "18600\n",
      "18700\n",
      "18800\n",
      "18900\n",
      "19000\n",
      "19100\n",
      "19200\n",
      "19300\n",
      "19400\n",
      "19500\n",
      "19600\n",
      "19700\n",
      "19800\n",
      "19900\n",
      "20000\n",
      "20100\n",
      "20200\n",
      "20300\n",
      "20400\n",
      "20500\n",
      "20600\n",
      "20700\n",
      "20800\n",
      "20900\n",
      "21000\n",
      "21100\n",
      "21200\n",
      "21300\n",
      "21400\n",
      "21500\n",
      "21600\n",
      "21700\n",
      "21800\n",
      "21900\n",
      "22000\n",
      "22100\n",
      "22200\n",
      "22300\n",
      "22400\n",
      "22500\n",
      "22600\n",
      "22700\n",
      "22800\n",
      "22900\n",
      "23000\n",
      "23100\n",
      "23200\n",
      "23300\n",
      "23400\n",
      "23500\n",
      "23600\n",
      "23700\n",
      "23800\n",
      "23900\n",
      "24000\n",
      "24100\n",
      "24200\n",
      "24300\n",
      "24400\n",
      "24500\n",
      "24600\n",
      "24700\n",
      "24800\n",
      "24900\n",
      "25000\n",
      "25100\n",
      "25200\n",
      "25300\n",
      "25400\n",
      "25500\n",
      "25600\n",
      "25700\n",
      "25800\n",
      "25900\n",
      "26000\n",
      "26100\n",
      "26200\n",
      "26300\n",
      "26400\n",
      "26500\n",
      "26600\n",
      "26700\n",
      "26800\n",
      "26900\n",
      "27000\n",
      "27100\n",
      "27200\n",
      "27300\n",
      "27400\n",
      "27500\n",
      "27600\n",
      "27700\n",
      "27800\n",
      "27900\n",
      "28000\n",
      "28100\n",
      "28200\n",
      "28300\n",
      "28400\n",
      "28500\n",
      "28600\n",
      "28700\n",
      "28800\n",
      "28900\n",
      "29000\n",
      "29100\n",
      "29200\n",
      "29300\n",
      "29400\n",
      "29500\n",
      "29600\n",
      "29700\n",
      "29800\n",
      "29900\n",
      "30000\n",
      "30100\n",
      "30200\n",
      "30300\n",
      "30400\n",
      "30500\n",
      "30600\n",
      "30700\n",
      "30800\n",
      "30900\n",
      "31000\n",
      "31100\n",
      "31200\n",
      "31300\n",
      "31400\n",
      "31500\n",
      "31600\n",
      "31700\n",
      "31800\n",
      "31900\n",
      "32000\n",
      "32100\n",
      "32200\n",
      "32300\n",
      "32400\n",
      "32500\n",
      "32600\n",
      "32700\n",
      "32800\n",
      "32900\n",
      "33000\n",
      "33100\n",
      "33200\n",
      "33300\n",
      "33400\n",
      "33500\n",
      "33600\n",
      "33700\n",
      "33800\n",
      "33900\n",
      "34000\n",
      "34100\n",
      "34200\n",
      "34300\n",
      "34400\n",
      "34500\n",
      "34600\n",
      "34700\n",
      "34800\n",
      "34900\n",
      "35000\n",
      "35100\n",
      "35200\n",
      "35300\n",
      "35400\n",
      "35500\n",
      "35600\n",
      "35700\n",
      "35800\n",
      "35900\n",
      "36000\n",
      "36100\n",
      "36200\n",
      "36300\n",
      "36400\n",
      "36500\n",
      "36600\n",
      "36700\n",
      "36800\n",
      "36900\n",
      "37000\n",
      "37100\n",
      "37200\n",
      "37300\n",
      "37400\n",
      "37500\n",
      "37600\n",
      "37700\n",
      "37800\n",
      "37900\n",
      "38000\n",
      "38100\n",
      "38200\n",
      "38300\n",
      "38400\n",
      "38500\n",
      "38600\n",
      "38700\n",
      "38800\n",
      "38900\n",
      "39000\n",
      "39100\n",
      "39200\n",
      "39300\n",
      "39400\n",
      "39500\n",
      "39600\n",
      "39700\n",
      "39800\n",
      "39900\n",
      "40000\n",
      "40100\n",
      "40200\n",
      "40300\n",
      "40400\n",
      "40500\n",
      "40600\n",
      "40700\n",
      "40800\n",
      "40900\n",
      "41000\n",
      "41100\n",
      "41200\n",
      "41300\n",
      "41400\n",
      "41500\n",
      "41600\n",
      "41700\n",
      "41800\n",
      "41900\n",
      "42000\n",
      "42100\n",
      "42200\n",
      "42300\n",
      "42400\n",
      "42500\n",
      "42600\n",
      "42700\n",
      "42800\n",
      "42900\n",
      "43000\n",
      "43100\n",
      "43200\n",
      "43300\n",
      "43400\n",
      "43500\n",
      "43600\n",
      "43700\n",
      "43800\n",
      "43900\n",
      "44000\n",
      "44100\n",
      "44200\n",
      "44300\n",
      "44400\n",
      "44500\n",
      "44600\n",
      "44700\n",
      "44800\n",
      "44900\n",
      "45000\n",
      "45100\n",
      "45200\n",
      "45300\n",
      "45400\n",
      "45500\n",
      "45600\n",
      "45700\n",
      "45800\n",
      "45900\n",
      "46000\n",
      "46100\n",
      "46200\n",
      "46300\n",
      "46400\n",
      "46500\n",
      "46600\n",
      "46700\n",
      "46800\n",
      "46900\n",
      "47000\n",
      "47100\n",
      "47200\n",
      "47300\n",
      "47400\n",
      "47500\n",
      "47600\n",
      "47700\n",
      "47800\n",
      "47900\n",
      "48000\n",
      "48100\n",
      "48200\n",
      "48300\n",
      "48400\n",
      "48500\n",
      "48600\n",
      "48700\n",
      "48800\n",
      "48900\n",
      "49000\n",
      "49100\n",
      "49200\n",
      "49300\n",
      "49400\n",
      "49500\n",
      "49600\n",
      "49700\n",
      "49800\n",
      "49900\n",
      "50000\n",
      "50100\n",
      "50200\n",
      "50300\n",
      "50400\n",
      "50500\n",
      "50600\n",
      "50700\n",
      "50800\n",
      "50900\n",
      "51000\n",
      "51100\n",
      "51200\n",
      "51300\n",
      "51400\n",
      "51500\n",
      "51600\n",
      "51700\n",
      "51800\n",
      "51900\n",
      "52000\n",
      "52100\n",
      "52200\n",
      "52300\n",
      "52400\n",
      "52500\n",
      "52600\n",
      "52700\n",
      "52800\n",
      "52900\n",
      "53000\n",
      "53100\n",
      "53200\n",
      "53300\n",
      "53400\n",
      "53500\n",
      "53600\n",
      "53700\n",
      "53800\n",
      "53900\n",
      "54000\n",
      "54100\n",
      "54200\n",
      "54300\n",
      "54400\n",
      "54500\n",
      "54600\n",
      "54700\n",
      "54800\n",
      "54900\n",
      "55000\n",
      "55100\n",
      "55200\n",
      "55300\n",
      "55400\n",
      "55500\n",
      "55600\n",
      "55700\n",
      "55800\n",
      "55900\n",
      "56000\n",
      "56100\n",
      "56200\n",
      "56300\n",
      "56400\n",
      "56500\n",
      "56600\n",
      "56700\n",
      "56800\n",
      "56900\n",
      "57000\n",
      "57100\n",
      "57200\n",
      "57300\n",
      "57400\n",
      "57500\n",
      "57600\n",
      "57700\n",
      "57800\n",
      "57900\n",
      "58000\n",
      "58100\n",
      "58200\n",
      "58300\n",
      "58400\n",
      "58500\n",
      "58600\n",
      "58700\n",
      "58800\n",
      "58900\n",
      "59000\n",
      "59100\n",
      "59200\n",
      "59300\n",
      "59400\n",
      "59500\n",
      "59600\n",
      "59700\n",
      "59800\n",
      "59900\n",
      "60000\n",
      "60100\n",
      "60200\n",
      "60300\n",
      "60400\n",
      "60500\n",
      "60600\n",
      "60700\n",
      "60800\n",
      "60900\n",
      "61000\n",
      "61100\n",
      "61200\n",
      "61300\n",
      "61400\n",
      "61500\n",
      "61600\n",
      "61700\n",
      "61800\n",
      "61900\n",
      "62000\n",
      "62100\n",
      "62200\n",
      "62300\n",
      "62400\n",
      "62500\n",
      "62600\n",
      "62700\n",
      "62800\n",
      "62900\n",
      "63000\n",
      "63100\n",
      "63200\n",
      "63300\n",
      "63400\n",
      "63500\n",
      "63600\n",
      "63700\n",
      "63800\n",
      "63900\n",
      "64000\n",
      "64100\n",
      "64200\n",
      "64300\n",
      "64400\n",
      "64500\n",
      "64600\n",
      "64700\n",
      "64800\n",
      "64900\n",
      "65000\n",
      "65100\n",
      "65200\n",
      "65300\n",
      "65400\n",
      "65500\n",
      "65600\n",
      "65700\n",
      "65800\n",
      "65900\n",
      "66000\n",
      "66100\n",
      "66200\n",
      "66300\n",
      "66400\n",
      "66500\n",
      "66600\n",
      "66700\n",
      "66800\n",
      "66900\n",
      "67000\n",
      "67100\n",
      "67200\n",
      "67300\n",
      "67400\n",
      "67500\n",
      "67600\n",
      "67700\n",
      "67800\n",
      "67900\n",
      "68000\n",
      "68100\n",
      "68200\n",
      "68300\n",
      "68400\n",
      "68500\n",
      "68600\n",
      "68700\n",
      "68800\n",
      "68900\n",
      "69000\n",
      "69100\n",
      "69200\n",
      "69300\n",
      "69400\n",
      "69500\n",
      "69600\n",
      "69700\n",
      "69800\n",
      "69900\n",
      "70000\n",
      "70100\n",
      "70200\n",
      "70300\n",
      "70400\n",
      "70500\n",
      "70600\n",
      "70700\n",
      "70800\n",
      "70900\n",
      "71000\n",
      "71100\n",
      "71200\n",
      "71300\n",
      "71400\n",
      "71500\n",
      "71600\n",
      "71700\n",
      "71800\n",
      "71900\n",
      "72000\n",
      "72100\n",
      "72200\n",
      "72300\n",
      "72400\n",
      "72500\n",
      "72600\n",
      "72700\n",
      "72800\n",
      "72900\n",
      "73000\n",
      "73100\n",
      "73200\n",
      "73300\n",
      "73400\n",
      "73500\n",
      "73600\n",
      "73700\n",
      "73800\n",
      "73900\n",
      "74000\n",
      "74100\n",
      "74200\n",
      "74300\n",
      "74400\n",
      "74500\n",
      "74600\n",
      "74700\n",
      "74800\n",
      "74900\n",
      "75000\n",
      "75100\n",
      "75200\n",
      "75300\n",
      "75400\n",
      "75500\n",
      "75600\n",
      "75700\n",
      "75800\n",
      "75900\n",
      "76000\n",
      "76100\n",
      "76200\n",
      "76300\n",
      "76400\n",
      "76500\n",
      "76600\n",
      "76700\n",
      "76800\n",
      "76900\n",
      "77000\n",
      "77100\n",
      "77200\n",
      "77300\n",
      "77400\n",
      "77500\n",
      "77600\n",
      "77700\n",
      "77800\n",
      "77900\n",
      "78000\n",
      "78100\n",
      "78200\n",
      "78300\n",
      "78400\n",
      "78500\n",
      "78600\n",
      "78700\n",
      "78800\n",
      "78900\n",
      "79000\n",
      "79100\n",
      "79200\n",
      "79300\n",
      "79400\n",
      "79500\n",
      "79600\n",
      "79700\n",
      "79800\n",
      "79900\n",
      "80000\n",
      "80100\n",
      "80200\n",
      "80300\n",
      "80400\n",
      "80500\n",
      "80600\n",
      "80700\n",
      "80800\n",
      "80900\n",
      "81000\n",
      "81100\n",
      "81200\n",
      "81300\n",
      "81400\n",
      "81500\n",
      "81600\n",
      "81700\n",
      "81800\n",
      "81900\n",
      "82000\n",
      "82100\n",
      "82200\n",
      "82300\n",
      "82400\n",
      "82500\n",
      "82600\n",
      "82700\n",
      "82800\n",
      "82900\n",
      "83000\n",
      "83100\n",
      "83200\n",
      "83300\n",
      "83400\n",
      "83500\n",
      "83600\n",
      "83700\n",
      "83800\n",
      "83900\n",
      "84000\n",
      "84100\n",
      "84200\n",
      "84300\n",
      "84400\n",
      "84500\n",
      "84600\n",
      "84700\n",
      "84800\n",
      "84900\n",
      "100\n",
      "Root directory: top_100_species_images/top_100_species_images\n",
      "Image count: 16994\n",
      "Label count: 16994\n"
     ]
    }
   ],
   "source": [
    "# Create dataset and dataloader objects for the training dataset\n",
    "dataset = CustomDataset(root_dir=root_dir, top_100_df=top_100_pairs, transform=transform)\n",
    "\n",
    "# Create DataLoaders to efficiently load data in batches\n",
    "train_loader = DataLoader(dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1374,
     "status": "ok",
     "timestamp": 1733743884759,
     "user": {
      "displayName": "Házifeladat Adatelemzés",
      "userId": "03777937392997268846"
     },
     "user_tz": -60
    },
    "id": "t2h5IIMzhdPA",
    "outputId": "58627da3-faad-44eb-d2bc-117d87bbcfa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] preparing model ...\n"
     ]
    }
   ],
   "source": [
    "# Define the model using pre-trained ResNet-50\n",
    "class MushroomClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=100):\n",
    "        super(MushroomClassifier, self).__init__()\n",
    "        self.model = models.resnet50(pretrained=True)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Instantiate the model and move it to GPU if available\n",
    "num_classes = 100\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = MushroomClassifier(num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1733743884760,
     "user": {
      "displayName": "Házifeladat Adatelemzés",
      "userId": "03777937392997268846"
     },
     "user_tz": -60
    },
    "id": "Fp-yMVNAhoDr"
   },
   "outputs": [],
   "source": [
    "# Set up the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BiWuybTpH0z"
   },
   "source": [
    "# KFold keresztvalidációs trainelése a modellnek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 240,
     "status": "ok",
     "timestamp": 1733745218330,
     "user": {
      "displayName": "Házifeladat Adatelemzés",
      "userId": "03777937392997268846"
     },
     "user_tz": -60
    },
    "id": "aPMNQazzpTU4"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader, SubsetRandomSampler\n",
    "import torch\n",
    "\n",
    "# Function to reset model weights\n",
    "def reset_weights(m):\n",
    "    if hasattr(m, 'reset_parameters'):\n",
    "        m.reset_parameters()\n",
    "\n",
    "# Define the train_kfold function\n",
    "def train_kfold(model, dataset, batch_size, num_folds, num_epochs, optimizer, criterion, device):\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    train_losses, val_losses, train_accuracies, val_accuracies, val_f1_scores = [], [], [], [], []\n",
    "    epoch_train_losses, epoch_val_losses, epoch_train_accuracies, epoch_val_accuracies, epoch_val_f1_scores = [], [], [], [], []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n",
    "        print(f\"Fold {fold + 1}/{num_folds}\")\n",
    "\n",
    "        train_sampler = SubsetRandomSampler(train_idx)\n",
    "        val_sampler = SubsetRandomSampler(val_idx)\n",
    "\n",
    "        train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n",
    "        val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n",
    "\n",
    "        model.apply(reset_weights)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            print(\"Itt vagyok epochban:\", epoch + 1)\n",
    "            model.train()\n",
    "            epoch_train_loss, epoch_train_acc = 0.0, 0.0\n",
    "\n",
    "            for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "\n",
    "                print(f\"Fold: {fold + 1}, epoch: {epoch + 1}, train batch: {batch_idx + 1}\")\n",
    "                inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "                epoch_train_loss += loss.item()\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                epoch_train_acc += torch.sum(preds == targets).item()\n",
    "\n",
    "                print(f\"Loss in batch: {loss.item()}\")\n",
    "                print(f\"Accuracy in batch: {torch.sum(preds == targets).item()}\")\n",
    "\n",
    "                train_losses.append(loss.item())\n",
    "                train_accuracies.append(torch.sum(preds == targets).item())\n",
    "\n",
    "            epoch_train_loss /= len(train_loader)\n",
    "            epoch_train_acc /= len(train_idx)\n",
    "\n",
    "            print(f\"Train loss: {epoch_train_loss}\")\n",
    "            print(f\"Train accuracy: {epoch_train_acc}\")\n",
    "\n",
    "            model.eval()\n",
    "            epoch_val_loss, epoch_val_acc = 0.0, 0.0\n",
    "            all_preds, all_targets = [], []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch_idx, (inputs, targets) in enumerate(val_loader):\n",
    "\n",
    "                    print(f\"Epoch: {epoch + 1}, validation batch: {batch_idx + 1}\")\n",
    "                    inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "                    outputs = model(inputs)\n",
    "                    loss = criterion(outputs, targets)\n",
    "                    epoch_val_loss += loss.item()\n",
    "\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    epoch_val_acc += torch.sum(preds == targets).item()\n",
    "\n",
    "                    all_preds.extend(preds.cpu().numpy())\n",
    "                    all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "                    val_losses.append(loss.item())\n",
    "                    val_accuracies.append(torch.sum(preds == targets).item())\n",
    "\n",
    "            epoch_val_loss /= len(val_loader)\n",
    "            epoch_val_acc /= len(val_idx)\n",
    "            epoch_val_f1 = f1_score(all_targets, all_preds, average='weighted')\n",
    "\n",
    "            print(f\"Validation loss: {epoch_val_loss}\")\n",
    "            print(f\"Validation accuracy: {epoch_val_acc}\")\n",
    "            print(f\"Validation F1 score: {epoch_val_f1}\")\n",
    "\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs} | \"\n",
    "                  f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.4f}, \"\n",
    "                  f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.4f}, Val F1: {epoch_val_f1:.4f}\")\n",
    "\n",
    "            epoch_train_losses.append(epoch_train_loss)\n",
    "            epoch_val_losses.append(epoch_val_loss)\n",
    "            epoch_train_accuracies.append(epoch_train_acc)\n",
    "            epoch_val_accuracies.append(epoch_val_acc)\n",
    "            epoch_val_f1_scores.append(epoch_val_f1)\n",
    "\n",
    "    return train_losses, val_losses, train_accuracies, val_accuracies, val_f1_scores, epoch_train_losses, epoch_val_losses, epoch_train_accuracies, epoch_val_accuracies, epoch_val_f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LPzg8ClytEmI",
    "outputId": "885ded93-2101-430a-b01f-1e7ae94e0ed9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/2\n",
      "Itt vagyok epochban: 1\n",
      "Fold: 1, epoch: 1, train batch: 1\n",
      "Loss in batch: 4.788154125213623\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 2\n",
      "Loss in batch: 7.275820732116699\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 3\n",
      "Loss in batch: 6.69547700881958\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 4\n",
      "Loss in batch: 6.218017578125\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 5\n",
      "Loss in batch: 7.429649353027344\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 6\n",
      "Loss in batch: 5.519186019897461\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 7\n",
      "Loss in batch: 5.514373779296875\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 8\n",
      "Loss in batch: 6.249794006347656\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 9\n",
      "Loss in batch: 5.3438825607299805\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 10\n",
      "Loss in batch: 6.266946792602539\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 11\n",
      "Loss in batch: 6.411480903625488\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 12\n",
      "Loss in batch: 5.421231746673584\n",
      "Accuracy in batch: 2\n",
      "Fold: 1, epoch: 1, train batch: 13\n",
      "Loss in batch: 5.1671528816223145\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 14\n",
      "Loss in batch: 5.4825119972229\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 15\n",
      "Loss in batch: 5.0697455406188965\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 16\n",
      "Loss in batch: 5.192476749420166\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 17\n",
      "Loss in batch: 4.7521514892578125\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 18\n",
      "Loss in batch: 5.420759201049805\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 19\n",
      "Loss in batch: 4.569155216217041\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 20\n",
      "Loss in batch: 5.096700191497803\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 21\n",
      "Loss in batch: 4.742429733276367\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 22\n",
      "Loss in batch: 5.582369327545166\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 23\n",
      "Loss in batch: 5.018461227416992\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 24\n",
      "Loss in batch: 5.227781295776367\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 25\n",
      "Loss in batch: 5.138646125793457\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 26\n",
      "Loss in batch: 4.503826141357422\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 27\n",
      "Loss in batch: 4.566699028015137\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 28\n",
      "Loss in batch: 4.9582839012146\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 29\n",
      "Loss in batch: 4.917984962463379\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 30\n",
      "Loss in batch: 5.011959075927734\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 31\n",
      "Loss in batch: 4.777379989624023\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 32\n",
      "Loss in batch: 4.865867614746094\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 33\n",
      "Loss in batch: 4.962457656860352\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 34\n",
      "Loss in batch: 5.478010654449463\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 35\n",
      "Loss in batch: 5.130245685577393\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 36\n",
      "Loss in batch: 5.075432300567627\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 37\n",
      "Loss in batch: 4.719352722167969\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 38\n",
      "Loss in batch: 4.65482759475708\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 39\n",
      "Loss in batch: 4.726737022399902\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 40\n",
      "Loss in batch: 4.821308612823486\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 41\n",
      "Loss in batch: 4.597542762756348\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 42\n",
      "Loss in batch: 4.869061470031738\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 43\n",
      "Loss in batch: 5.054790496826172\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 44\n",
      "Loss in batch: 4.893293380737305\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 45\n",
      "Loss in batch: 5.0883073806762695\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 46\n",
      "Loss in batch: 4.769960403442383\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 47\n",
      "Loss in batch: 4.693447113037109\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 48\n",
      "Loss in batch: 5.092230796813965\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 49\n",
      "Loss in batch: 4.695688724517822\n",
      "Accuracy in batch: 3\n",
      "Fold: 1, epoch: 1, train batch: 50\n",
      "Loss in batch: 4.375821113586426\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 51\n",
      "Loss in batch: 5.247994899749756\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 52\n",
      "Loss in batch: 4.747364044189453\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 53\n",
      "Loss in batch: 4.801996231079102\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 54\n",
      "Loss in batch: 4.659915924072266\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 55\n",
      "Loss in batch: 4.8839521408081055\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 56\n",
      "Loss in batch: 4.824561595916748\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 57\n",
      "Loss in batch: 4.989893436431885\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 58\n",
      "Loss in batch: 4.731678485870361\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 59\n",
      "Loss in batch: 5.244875907897949\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 60\n",
      "Loss in batch: 4.712662220001221\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 61\n",
      "Loss in batch: 4.94681978225708\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 62\n",
      "Loss in batch: 5.058407783508301\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 63\n",
      "Loss in batch: 5.001342296600342\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 64\n",
      "Loss in batch: 4.67223596572876\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 65\n",
      "Loss in batch: 4.717173099517822\n",
      "Accuracy in batch: 2\n",
      "Fold: 1, epoch: 1, train batch: 66\n",
      "Loss in batch: 4.759002208709717\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 67\n",
      "Loss in batch: 5.098094463348389\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 68\n",
      "Loss in batch: 4.845468521118164\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 69\n",
      "Loss in batch: 4.849246025085449\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 70\n",
      "Loss in batch: 4.710820198059082\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 71\n",
      "Loss in batch: 4.770525932312012\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 72\n",
      "Loss in batch: 4.812796592712402\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 73\n",
      "Loss in batch: 4.887823104858398\n",
      "Accuracy in batch: 2\n",
      "Fold: 1, epoch: 1, train batch: 74\n",
      "Loss in batch: 4.592149257659912\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 75\n",
      "Loss in batch: 5.007246017456055\n",
      "Accuracy in batch: 2\n",
      "Fold: 1, epoch: 1, train batch: 76\n",
      "Loss in batch: 4.8424973487854\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 77\n",
      "Loss in batch: 4.695858955383301\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 78\n",
      "Loss in batch: 4.813443183898926\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 79\n",
      "Loss in batch: 4.610394477844238\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 80\n",
      "Loss in batch: 5.187320709228516\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 81\n",
      "Loss in batch: 4.944264888763428\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 82\n",
      "Loss in batch: 4.740272045135498\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 83\n",
      "Loss in batch: 4.956191062927246\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 84\n",
      "Loss in batch: 4.675736427307129\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 85\n",
      "Loss in batch: 4.583822250366211\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 86\n",
      "Loss in batch: 4.574511528015137\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 87\n",
      "Loss in batch: 4.588520050048828\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 88\n",
      "Loss in batch: 4.625086307525635\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 89\n",
      "Loss in batch: 4.6324639320373535\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 90\n",
      "Loss in batch: 4.4476141929626465\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 91\n",
      "Loss in batch: 4.689167022705078\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 92\n",
      "Loss in batch: 4.567020416259766\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 93\n",
      "Loss in batch: 4.573333263397217\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 94\n",
      "Loss in batch: 4.545738697052002\n",
      "Accuracy in batch: 2\n",
      "Fold: 1, epoch: 1, train batch: 95\n",
      "Loss in batch: 4.628602504730225\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 96\n",
      "Loss in batch: 4.408764362335205\n",
      "Accuracy in batch: 2\n",
      "Fold: 1, epoch: 1, train batch: 97\n",
      "Loss in batch: 4.668863296508789\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 98\n",
      "Loss in batch: 4.759172439575195\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 99\n",
      "Loss in batch: 4.723670959472656\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 100\n",
      "Loss in batch: 4.651974678039551\n",
      "Accuracy in batch: 2\n",
      "Fold: 1, epoch: 1, train batch: 101\n",
      "Loss in batch: 4.700192928314209\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 102\n",
      "Loss in batch: 4.8074541091918945\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 103\n",
      "Loss in batch: 4.468399524688721\n",
      "Accuracy in batch: 2\n",
      "Fold: 1, epoch: 1, train batch: 104\n",
      "Loss in batch: 4.708189487457275\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 105\n",
      "Loss in batch: 4.771183967590332\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 106\n",
      "Loss in batch: 4.582596778869629\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 107\n",
      "Loss in batch: 4.613915920257568\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 108\n",
      "Loss in batch: 4.577919960021973\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 109\n",
      "Loss in batch: 4.543938159942627\n",
      "Accuracy in batch: 2\n",
      "Fold: 1, epoch: 1, train batch: 110\n",
      "Loss in batch: 4.569235801696777\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 111\n",
      "Loss in batch: 4.750967979431152\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 112\n",
      "Loss in batch: 4.909821033477783\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 113\n",
      "Loss in batch: 4.514318943023682\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 114\n",
      "Loss in batch: 4.756094455718994\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 115\n",
      "Loss in batch: 4.550876617431641\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 116\n",
      "Loss in batch: 4.6369147300720215\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 117\n",
      "Loss in batch: 4.710104465484619\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 118\n",
      "Loss in batch: 4.489729404449463\n",
      "Accuracy in batch: 3\n",
      "Fold: 1, epoch: 1, train batch: 119\n",
      "Loss in batch: 4.747751235961914\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 120\n",
      "Loss in batch: 4.5252885818481445\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 121\n",
      "Loss in batch: 4.65433406829834\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 122\n",
      "Loss in batch: 4.893376350402832\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 123\n",
      "Loss in batch: 4.748861789703369\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 124\n",
      "Loss in batch: 4.6316447257995605\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 125\n",
      "Loss in batch: 4.509561061859131\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 126\n",
      "Loss in batch: 4.567011833190918\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 127\n",
      "Loss in batch: 4.757091999053955\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 128\n",
      "Loss in batch: 4.610350608825684\n",
      "Accuracy in batch: 0\n",
      "Fold: 1, epoch: 1, train batch: 129\n",
      "Loss in batch: 4.5363078117370605\n",
      "Accuracy in batch: 2\n",
      "Fold: 1, epoch: 1, train batch: 130\n",
      "Loss in batch: 4.634617328643799\n",
      "Accuracy in batch: 3\n",
      "Fold: 1, epoch: 1, train batch: 131\n",
      "Loss in batch: 4.519462585449219\n",
      "Accuracy in batch: 2\n",
      "Fold: 1, epoch: 1, train batch: 132\n",
      "Loss in batch: 4.396470546722412\n",
      "Accuracy in batch: 1\n",
      "Fold: 1, epoch: 1, train batch: 133\n",
      "Loss in batch: 4.417921543121338\n",
      "Accuracy in batch: 2\n",
      "Train loss: 4.9053769147485715\n",
      "Train accuracy: 0.014595103578154425\n",
      "Epoch: 1, validation batch: 1\n",
      "Epoch: 1, validation batch: 2\n",
      "Epoch: 1, validation batch: 3\n",
      "Epoch: 1, validation batch: 4\n",
      "Epoch: 1, validation batch: 5\n",
      "Epoch: 1, validation batch: 6\n",
      "Epoch: 1, validation batch: 7\n",
      "Epoch: 1, validation batch: 8\n",
      "Epoch: 1, validation batch: 9\n",
      "Epoch: 1, validation batch: 10\n",
      "Epoch: 1, validation batch: 11\n",
      "Epoch: 1, validation batch: 12\n",
      "Epoch: 1, validation batch: 13\n",
      "Epoch: 1, validation batch: 14\n",
      "Epoch: 1, validation batch: 15\n",
      "Epoch: 1, validation batch: 16\n",
      "Epoch: 1, validation batch: 17\n",
      "Epoch: 1, validation batch: 18\n",
      "Epoch: 1, validation batch: 19\n",
      "Epoch: 1, validation batch: 20\n",
      "Epoch: 1, validation batch: 21\n",
      "Epoch: 1, validation batch: 22\n",
      "Epoch: 1, validation batch: 23\n",
      "Epoch: 1, validation batch: 24\n",
      "Epoch: 1, validation batch: 25\n",
      "Epoch: 1, validation batch: 26\n",
      "Epoch: 1, validation batch: 27\n",
      "Epoch: 1, validation batch: 28\n",
      "Epoch: 1, validation batch: 29\n",
      "Epoch: 1, validation batch: 30\n",
      "Epoch: 1, validation batch: 31\n",
      "Epoch: 1, validation batch: 32\n",
      "Epoch: 1, validation batch: 33\n",
      "Epoch: 1, validation batch: 34\n",
      "Epoch: 1, validation batch: 35\n",
      "Epoch: 1, validation batch: 36\n",
      "Epoch: 1, validation batch: 37\n",
      "Epoch: 1, validation batch: 38\n",
      "Epoch: 1, validation batch: 39\n",
      "Epoch: 1, validation batch: 40\n",
      "Epoch: 1, validation batch: 41\n",
      "Epoch: 1, validation batch: 42\n",
      "Epoch: 1, validation batch: 43\n",
      "Epoch: 1, validation batch: 44\n",
      "Epoch: 1, validation batch: 45\n",
      "Epoch: 1, validation batch: 46\n",
      "Epoch: 1, validation batch: 47\n",
      "Epoch: 1, validation batch: 48\n",
      "Epoch: 1, validation batch: 49\n",
      "Epoch: 1, validation batch: 50\n",
      "Epoch: 1, validation batch: 51\n",
      "Epoch: 1, validation batch: 52\n",
      "Epoch: 1, validation batch: 53\n",
      "Epoch: 1, validation batch: 54\n",
      "Epoch: 1, validation batch: 55\n",
      "Epoch: 1, validation batch: 56\n",
      "Epoch: 1, validation batch: 57\n",
      "Epoch: 1, validation batch: 58\n",
      "Epoch: 1, validation batch: 59\n",
      "Epoch: 1, validation batch: 60\n",
      "Epoch: 1, validation batch: 61\n",
      "Epoch: 1, validation batch: 62\n",
      "Epoch: 1, validation batch: 63\n",
      "Epoch: 1, validation batch: 64\n",
      "Epoch: 1, validation batch: 65\n",
      "Epoch: 1, validation batch: 66\n",
      "Epoch: 1, validation batch: 67\n",
      "Epoch: 1, validation batch: 68\n",
      "Epoch: 1, validation batch: 69\n",
      "Epoch: 1, validation batch: 70\n",
      "Epoch: 1, validation batch: 71\n",
      "Epoch: 1, validation batch: 72\n",
      "Epoch: 1, validation batch: 73\n",
      "Epoch: 1, validation batch: 74\n",
      "Epoch: 1, validation batch: 75\n",
      "Epoch: 1, validation batch: 76\n",
      "Epoch: 1, validation batch: 77\n",
      "Epoch: 1, validation batch: 78\n",
      "Epoch: 1, validation batch: 79\n",
      "Epoch: 1, validation batch: 80\n",
      "Epoch: 1, validation batch: 81\n",
      "Epoch: 1, validation batch: 82\n",
      "Epoch: 1, validation batch: 83\n",
      "Epoch: 1, validation batch: 84\n",
      "Epoch: 1, validation batch: 85\n",
      "Epoch: 1, validation batch: 86\n",
      "Epoch: 1, validation batch: 87\n",
      "Epoch: 1, validation batch: 88\n",
      "Epoch: 1, validation batch: 89\n",
      "Epoch: 1, validation batch: 90\n",
      "Epoch: 1, validation batch: 91\n",
      "Epoch: 1, validation batch: 92\n",
      "Epoch: 1, validation batch: 93\n",
      "Epoch: 1, validation batch: 94\n",
      "Epoch: 1, validation batch: 95\n",
      "Epoch: 1, validation batch: 96\n",
      "Epoch: 1, validation batch: 97\n",
      "Epoch: 1, validation batch: 98\n",
      "Epoch: 1, validation batch: 99\n",
      "Epoch: 1, validation batch: 100\n",
      "Epoch: 1, validation batch: 101\n",
      "Epoch: 1, validation batch: 102\n",
      "Epoch: 1, validation batch: 103\n",
      "Epoch: 1, validation batch: 104\n",
      "Epoch: 1, validation batch: 105\n",
      "Epoch: 1, validation batch: 106\n",
      "Epoch: 1, validation batch: 107\n",
      "Epoch: 1, validation batch: 108\n",
      "Epoch: 1, validation batch: 109\n",
      "Epoch: 1, validation batch: 110\n",
      "Epoch: 1, validation batch: 111\n",
      "Epoch: 1, validation batch: 112\n",
      "Epoch: 1, validation batch: 113\n",
      "Epoch: 1, validation batch: 114\n",
      "Epoch: 1, validation batch: 115\n",
      "Epoch: 1, validation batch: 116\n",
      "Epoch: 1, validation batch: 117\n",
      "Epoch: 1, validation batch: 118\n",
      "Epoch: 1, validation batch: 119\n",
      "Epoch: 1, validation batch: 120\n",
      "Epoch: 1, validation batch: 121\n",
      "Epoch: 1, validation batch: 122\n",
      "Epoch: 1, validation batch: 123\n",
      "Epoch: 1, validation batch: 124\n",
      "Epoch: 1, validation batch: 125\n",
      "Epoch: 1, validation batch: 126\n",
      "Epoch: 1, validation batch: 127\n",
      "Epoch: 1, validation batch: 128\n",
      "Epoch: 1, validation batch: 129\n",
      "Epoch: 1, validation batch: 130\n",
      "Epoch: 1, validation batch: 131\n",
      "Epoch: 1, validation batch: 132\n",
      "Epoch: 1, validation batch: 133\n",
      "Validation loss: 4.651090833477508\n",
      "Validation accuracy: 0.030595434219816427\n",
      "Validation F1 score: 0.00415433511694822\n",
      "Epoch 1/1 | Train Loss: 4.9054, Train Acc: 0.0146, Val Loss: 4.6511, Val Acc: 0.0306, Val F1: 0.0042\n",
      "Fold 2/2\n",
      "Itt vagyok epochban: 1\n",
      "Fold: 2, epoch: 1, train batch: 1\n",
      "Loss in batch: 4.973818778991699\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 2\n",
      "Loss in batch: 5.124946117401123\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 3\n",
      "Loss in batch: 6.3618879318237305\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 4\n",
      "Loss in batch: 6.085355281829834\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 5\n",
      "Loss in batch: 6.178267955780029\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 6\n",
      "Loss in batch: 5.537769794464111\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 7\n",
      "Loss in batch: 5.632257461547852\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 8\n",
      "Loss in batch: 6.866175174713135\n",
      "Accuracy in batch: 2\n",
      "Fold: 2, epoch: 1, train batch: 9\n",
      "Loss in batch: 5.50175666809082\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 10\n",
      "Loss in batch: 5.787633895874023\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 11\n",
      "Loss in batch: 6.129720687866211\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 12\n",
      "Loss in batch: 5.581000804901123\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 13\n",
      "Loss in batch: 6.287249565124512\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 14\n",
      "Loss in batch: 5.520145416259766\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 15\n",
      "Loss in batch: 5.860405445098877\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 16\n",
      "Loss in batch: 5.179230690002441\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 17\n",
      "Loss in batch: 4.950133323669434\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 18\n",
      "Loss in batch: 5.587487697601318\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 19\n",
      "Loss in batch: 5.513453960418701\n",
      "Accuracy in batch: 2\n",
      "Fold: 2, epoch: 1, train batch: 20\n",
      "Loss in batch: 4.783748626708984\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 21\n",
      "Loss in batch: 5.164705276489258\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 22\n",
      "Loss in batch: 5.245349884033203\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 23\n",
      "Loss in batch: 5.634006977081299\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 24\n",
      "Loss in batch: 5.5118727684021\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 25\n",
      "Loss in batch: 4.724055290222168\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 26\n",
      "Loss in batch: 5.332351207733154\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 27\n",
      "Loss in batch: 5.410125732421875\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 28\n",
      "Loss in batch: 5.135885238647461\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 29\n",
      "Loss in batch: 5.2926411628723145\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 30\n",
      "Loss in batch: 4.702897548675537\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 31\n",
      "Loss in batch: 5.0104875564575195\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 32\n",
      "Loss in batch: 4.880532264709473\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 33\n",
      "Loss in batch: 4.332170486450195\n",
      "Accuracy in batch: 5\n",
      "Fold: 2, epoch: 1, train batch: 34\n",
      "Loss in batch: 5.386312961578369\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 35\n",
      "Loss in batch: 4.949315071105957\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 36\n",
      "Loss in batch: 5.458149433135986\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 37\n",
      "Loss in batch: 5.127390384674072\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 38\n",
      "Loss in batch: 5.024840354919434\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 39\n",
      "Loss in batch: 4.770900726318359\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 40\n",
      "Loss in batch: 5.049958229064941\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 41\n",
      "Loss in batch: 5.259979248046875\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 42\n",
      "Loss in batch: 5.0297369956970215\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 43\n",
      "Loss in batch: 4.75520133972168\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 44\n",
      "Loss in batch: 5.019737720489502\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 45\n",
      "Loss in batch: 4.934008598327637\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 46\n",
      "Loss in batch: 5.137109756469727\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 47\n",
      "Loss in batch: 4.922900676727295\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 48\n",
      "Loss in batch: 5.149759292602539\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 49\n",
      "Loss in batch: 4.640049457550049\n",
      "Accuracy in batch: 2\n",
      "Fold: 2, epoch: 1, train batch: 50\n",
      "Loss in batch: 4.769191741943359\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 51\n",
      "Loss in batch: 4.591156482696533\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 52\n",
      "Loss in batch: 4.822649002075195\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 53\n",
      "Loss in batch: 4.906102180480957\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 54\n",
      "Loss in batch: 4.840601921081543\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 55\n",
      "Loss in batch: 4.740216255187988\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 56\n",
      "Loss in batch: 4.716243267059326\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 57\n",
      "Loss in batch: 4.932147026062012\n",
      "Accuracy in batch: 2\n",
      "Fold: 2, epoch: 1, train batch: 58\n",
      "Loss in batch: 4.743164539337158\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 59\n",
      "Loss in batch: 4.808273792266846\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 60\n",
      "Loss in batch: 4.804649353027344\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 61\n",
      "Loss in batch: 4.565728664398193\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 62\n",
      "Loss in batch: 4.768045902252197\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 63\n",
      "Loss in batch: 4.982616901397705\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 64\n",
      "Loss in batch: 4.727720260620117\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 65\n",
      "Loss in batch: 4.916877746582031\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 66\n",
      "Loss in batch: 4.712428569793701\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 67\n",
      "Loss in batch: 4.856647491455078\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 68\n",
      "Loss in batch: 5.133162021636963\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 69\n",
      "Loss in batch: 4.72723913192749\n",
      "Accuracy in batch: 3\n",
      "Fold: 2, epoch: 1, train batch: 70\n",
      "Loss in batch: 4.789689064025879\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 71\n",
      "Loss in batch: 4.736618995666504\n",
      "Accuracy in batch: 3\n",
      "Fold: 2, epoch: 1, train batch: 72\n",
      "Loss in batch: 4.723140239715576\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 73\n",
      "Loss in batch: 4.641427040100098\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 74\n",
      "Loss in batch: 4.596993923187256\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 75\n",
      "Loss in batch: 4.534688472747803\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 76\n",
      "Loss in batch: 4.748291492462158\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 77\n",
      "Loss in batch: 4.508692264556885\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 78\n",
      "Loss in batch: 4.803595542907715\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 79\n",
      "Loss in batch: 4.7963175773620605\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 80\n",
      "Loss in batch: 4.885492324829102\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 81\n",
      "Loss in batch: 4.760095596313477\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 82\n",
      "Loss in batch: 4.802856922149658\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 83\n",
      "Loss in batch: 4.844715595245361\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 84\n",
      "Loss in batch: 4.712674140930176\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 85\n",
      "Loss in batch: 5.00120735168457\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 86\n",
      "Loss in batch: 4.971185207366943\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 87\n",
      "Loss in batch: 4.867564678192139\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 88\n",
      "Loss in batch: 4.656932353973389\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 89\n",
      "Loss in batch: 4.760152339935303\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 90\n",
      "Loss in batch: 4.703634262084961\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 91\n",
      "Loss in batch: 4.717777252197266\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 92\n",
      "Loss in batch: 4.774351596832275\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 93\n",
      "Loss in batch: 4.694202423095703\n",
      "Accuracy in batch: 3\n",
      "Fold: 2, epoch: 1, train batch: 94\n",
      "Loss in batch: 4.623020172119141\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 95\n",
      "Loss in batch: 5.165216445922852\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 96\n",
      "Loss in batch: 4.741128921508789\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 97\n",
      "Loss in batch: 4.578066349029541\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 98\n",
      "Loss in batch: 4.628574848175049\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 99\n",
      "Loss in batch: 4.70936393737793\n",
      "Accuracy in batch: 2\n",
      "Fold: 2, epoch: 1, train batch: 100\n",
      "Loss in batch: 4.83105993270874\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 101\n",
      "Loss in batch: 4.504485130310059\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 102\n",
      "Loss in batch: 4.7196574211120605\n",
      "Accuracy in batch: 2\n",
      "Fold: 2, epoch: 1, train batch: 103\n",
      "Loss in batch: 4.637258529663086\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 104\n",
      "Loss in batch: 4.745430946350098\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 105\n",
      "Loss in batch: 4.547636985778809\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 106\n",
      "Loss in batch: 4.540884494781494\n",
      "Accuracy in batch: 2\n",
      "Fold: 2, epoch: 1, train batch: 107\n",
      "Loss in batch: 4.505370140075684\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 108\n",
      "Loss in batch: 4.65349817276001\n",
      "Accuracy in batch: 2\n",
      "Fold: 2, epoch: 1, train batch: 109\n",
      "Loss in batch: 4.765270709991455\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 110\n",
      "Loss in batch: 4.636350631713867\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 111\n",
      "Loss in batch: 4.722110748291016\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 112\n",
      "Loss in batch: 4.962179183959961\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 113\n",
      "Loss in batch: 4.639112949371338\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 114\n",
      "Loss in batch: 4.630871772766113\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 115\n",
      "Loss in batch: 4.6246018409729\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 116\n",
      "Loss in batch: 4.831676483154297\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 117\n",
      "Loss in batch: 4.817324638366699\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 118\n",
      "Loss in batch: 4.613864421844482\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 119\n",
      "Loss in batch: 4.80057430267334\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 120\n",
      "Loss in batch: 4.598165035247803\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 121\n",
      "Loss in batch: 4.9771223068237305\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 122\n",
      "Loss in batch: 4.873013496398926\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 123\n",
      "Loss in batch: 4.6179728507995605\n",
      "Accuracy in batch: 3\n",
      "Fold: 2, epoch: 1, train batch: 124\n",
      "Loss in batch: 4.735766887664795\n",
      "Accuracy in batch: 2\n",
      "Fold: 2, epoch: 1, train batch: 125\n",
      "Loss in batch: 4.759378910064697\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 126\n",
      "Loss in batch: 4.723757743835449\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 127\n",
      "Loss in batch: 4.755002975463867\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 128\n",
      "Loss in batch: 4.828004837036133\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 129\n",
      "Loss in batch: 4.75665283203125\n",
      "Accuracy in batch: 0\n",
      "Fold: 2, epoch: 1, train batch: 130\n",
      "Loss in batch: 4.51030158996582\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 131\n",
      "Loss in batch: 4.702558517456055\n",
      "Accuracy in batch: 2\n",
      "Fold: 2, epoch: 1, train batch: 132\n",
      "Loss in batch: 4.566725730895996\n",
      "Accuracy in batch: 1\n",
      "Fold: 2, epoch: 1, train batch: 133\n",
      "Loss in batch: 4.754255294799805\n",
      "Accuracy in batch: 0\n",
      "Train loss: 4.953874473284958\n",
      "Train accuracy: 0.016003765591903977\n",
      "Epoch: 1, validation batch: 1\n",
      "Epoch: 1, validation batch: 2\n",
      "Epoch: 1, validation batch: 3\n",
      "Epoch: 1, validation batch: 4\n",
      "Epoch: 1, validation batch: 5\n",
      "Epoch: 1, validation batch: 6\n",
      "Epoch: 1, validation batch: 7\n",
      "Epoch: 1, validation batch: 8\n",
      "Epoch: 1, validation batch: 9\n",
      "Epoch: 1, validation batch: 10\n",
      "Epoch: 1, validation batch: 11\n",
      "Epoch: 1, validation batch: 12\n",
      "Epoch: 1, validation batch: 13\n",
      "Epoch: 1, validation batch: 14\n",
      "Epoch: 1, validation batch: 15\n",
      "Epoch: 1, validation batch: 16\n",
      "Epoch: 1, validation batch: 17\n",
      "Epoch: 1, validation batch: 18\n",
      "Epoch: 1, validation batch: 19\n",
      "Epoch: 1, validation batch: 20\n",
      "Epoch: 1, validation batch: 21\n",
      "Epoch: 1, validation batch: 22\n",
      "Epoch: 1, validation batch: 23\n",
      "Epoch: 1, validation batch: 24\n",
      "Epoch: 1, validation batch: 25\n",
      "Epoch: 1, validation batch: 26\n",
      "Epoch: 1, validation batch: 27\n",
      "Epoch: 1, validation batch: 28\n",
      "Epoch: 1, validation batch: 29\n",
      "Epoch: 1, validation batch: 30\n",
      "Epoch: 1, validation batch: 31\n",
      "Epoch: 1, validation batch: 32\n",
      "Epoch: 1, validation batch: 33\n",
      "Epoch: 1, validation batch: 34\n",
      "Epoch: 1, validation batch: 35\n",
      "Epoch: 1, validation batch: 36\n",
      "Epoch: 1, validation batch: 37\n",
      "Epoch: 1, validation batch: 38\n",
      "Epoch: 1, validation batch: 39\n",
      "Epoch: 1, validation batch: 40\n",
      "Epoch: 1, validation batch: 41\n",
      "Epoch: 1, validation batch: 42\n",
      "Epoch: 1, validation batch: 43\n",
      "Epoch: 1, validation batch: 44\n",
      "Epoch: 1, validation batch: 45\n",
      "Epoch: 1, validation batch: 46\n",
      "Epoch: 1, validation batch: 47\n",
      "Epoch: 1, validation batch: 48\n",
      "Epoch: 1, validation batch: 49\n",
      "Epoch: 1, validation batch: 50\n",
      "Epoch: 1, validation batch: 51\n",
      "Epoch: 1, validation batch: 52\n",
      "Epoch: 1, validation batch: 53\n",
      "Epoch: 1, validation batch: 54\n",
      "Epoch: 1, validation batch: 55\n",
      "Epoch: 1, validation batch: 56\n",
      "Epoch: 1, validation batch: 57\n",
      "Epoch: 1, validation batch: 58\n",
      "Epoch: 1, validation batch: 59\n",
      "Epoch: 1, validation batch: 60\n",
      "Epoch: 1, validation batch: 61\n",
      "Epoch: 1, validation batch: 62\n",
      "Epoch: 1, validation batch: 63\n",
      "Epoch: 1, validation batch: 64\n",
      "Epoch: 1, validation batch: 65\n",
      "Epoch: 1, validation batch: 66\n",
      "Epoch: 1, validation batch: 67\n",
      "Epoch: 1, validation batch: 68\n",
      "Epoch: 1, validation batch: 69\n",
      "Epoch: 1, validation batch: 70\n",
      "Epoch: 1, validation batch: 71\n",
      "Epoch: 1, validation batch: 72\n",
      "Epoch: 1, validation batch: 73\n",
      "Epoch: 1, validation batch: 74\n",
      "Epoch: 1, validation batch: 75\n",
      "Epoch: 1, validation batch: 76\n",
      "Epoch: 1, validation batch: 77\n",
      "Epoch: 1, validation batch: 78\n",
      "Epoch: 1, validation batch: 79\n",
      "Epoch: 1, validation batch: 80\n",
      "Epoch: 1, validation batch: 81\n",
      "Epoch: 1, validation batch: 82\n",
      "Epoch: 1, validation batch: 83\n",
      "Epoch: 1, validation batch: 84\n",
      "Epoch: 1, validation batch: 85\n",
      "Epoch: 1, validation batch: 86\n",
      "Epoch: 1, validation batch: 87\n",
      "Epoch: 1, validation batch: 88\n",
      "Epoch: 1, validation batch: 89\n",
      "Epoch: 1, validation batch: 90\n",
      "Epoch: 1, validation batch: 91\n",
      "Epoch: 1, validation batch: 92\n",
      "Epoch: 1, validation batch: 93\n",
      "Epoch: 1, validation batch: 94\n",
      "Epoch: 1, validation batch: 95\n",
      "Epoch: 1, validation batch: 96\n",
      "Epoch: 1, validation batch: 97\n",
      "Epoch: 1, validation batch: 98\n",
      "Epoch: 1, validation batch: 99\n",
      "Epoch: 1, validation batch: 100\n",
      "Epoch: 1, validation batch: 101\n",
      "Epoch: 1, validation batch: 102\n",
      "Epoch: 1, validation batch: 103\n",
      "Epoch: 1, validation batch: 104\n",
      "Epoch: 1, validation batch: 105\n",
      "Epoch: 1, validation batch: 106\n",
      "Epoch: 1, validation batch: 107\n",
      "Epoch: 1, validation batch: 108\n",
      "Epoch: 1, validation batch: 109\n",
      "Epoch: 1, validation batch: 110\n",
      "Epoch: 1, validation batch: 111\n",
      "Epoch: 1, validation batch: 112\n",
      "Epoch: 1, validation batch: 113\n",
      "Epoch: 1, validation batch: 114\n",
      "Epoch: 1, validation batch: 115\n",
      "Epoch: 1, validation batch: 116\n",
      "Epoch: 1, validation batch: 117\n",
      "Epoch: 1, validation batch: 118\n",
      "Epoch: 1, validation batch: 119\n",
      "Epoch: 1, validation batch: 120\n",
      "Epoch: 1, validation batch: 121\n",
      "Epoch: 1, validation batch: 122\n",
      "Epoch: 1, validation batch: 123\n",
      "Epoch: 1, validation batch: 124\n",
      "Epoch: 1, validation batch: 125\n",
      "Epoch: 1, validation batch: 126\n",
      "Epoch: 1, validation batch: 127\n",
      "Epoch: 1, validation batch: 128\n",
      "Epoch: 1, validation batch: 129\n",
      "Epoch: 1, validation batch: 130\n",
      "Epoch: 1, validation batch: 131\n",
      "Epoch: 1, validation batch: 132\n",
      "Epoch: 1, validation batch: 133\n",
      "Validation loss: 4.629082432366852\n",
      "Validation accuracy: 0.01694915254237288\n",
      "Validation F1 score: 0.003333619676626226\n",
      "Epoch 1/1 | Train Loss: 4.9539, Train Acc: 0.0160, Val Loss: 4.6291, Val Acc: 0.0169, Val F1: 0.0033\n"
     ]
    }
   ],
   "source": [
    "# perform the first training session\n",
    "train_losses, val_losses, train_accuracies, val_accuracies, val_f1_scores, epoch_train_losses, epoch_val_losses, epoch_train_accuracies, epoch_val_accuracies, epoch_val_f1_scores = train_kfold(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    batch_size= 32,\n",
    "    num_folds= 2,\n",
    "    num_epochs= 1,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# save our model\n",
    "torch.save(model.state_dict(), 'resnet50_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the model again and perform further training sessions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load our model from the saved state dictionary\n",
    "model = MushroomClassifier()\n",
    "model.load_state_dict(torch.load('resnet50_model.pth'))\n",
    "model.eval()\n",
    "\n",
    "# Train further to finetune the model\n",
    "train_losses, val_losses, train_accuracies, val_accuracies, val_f1_scores, epoch_train_losses, epoch_val_losses, epoch_train_accuracies, epoch_val_accuracies, epoch_val_f1_scores = train_kfold(\n",
    "    model=model,\n",
    "    dataset=dataset,\n",
    "    batch_size= 32,\n",
    "    num_folds= 3,\n",
    "    num_epochs= 2,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "# Save it again\n",
    "torch.save(model.state_dict(), 'resnet50_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2130\n",
      "1066\n",
      "2130\n",
      "1066\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary with the lists\n",
    "data = {\n",
    "    'train_losses': epoch_train_losses,\n",
    "    #'val_losses': val_losses,\n",
    "    'train_accuracies': epoch_train_accuracies,\n",
    "    #'val_accuracies': val_accuracies,\n",
    "}\n",
    "\n",
    "print(len(train_losses))\n",
    "print(len(val_losses))\n",
    "print(len(train_accuracies))\n",
    "print(len(val_accuracies))\n",
    "\n",
    "# Convert the dictionary to a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Save the DataFrame as a CSV file\n",
    "df.to_csv('training_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyPGy+hLp958SQdYd7wRvWM+",
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
